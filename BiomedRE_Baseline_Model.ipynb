{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e45c6b2",
   "metadata": {},
   "source": [
    "# Biomedical Relation Extraction from Scientific Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068f741",
   "metadata": {},
   "source": [
    "Baseline BERT model to extract relationships from PubMed articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0824c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch, logging\n",
    "\n",
    "# fix random seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# CUDA device if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "# log level for experiment\n",
    "logger = logging.getLogger(\"BioRE\")\n",
    "\n",
    "# code for the baseline model\n",
    "sys.path.append(\"./baseline/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ba182f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mramonreszat\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ramon/Developer/BiomedRE/wandb/run-20231019_002220-73cgfrdw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ramonreszat/biomed-bert-re/runs/73cgfrdw' target=\"_blank\">twilight-dawn-1</a></strong> to <a href='https://wandb.ai/ramonreszat/biomed-bert-re' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ramonreszat/biomed-bert-re' target=\"_blank\">https://wandb.ai/ramonreszat/biomed-bert-re</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ramonreszat/biomed-bert-re/runs/73cgfrdw' target=\"_blank\">https://wandb.ai/ramonreszat/biomed-bert-re/runs/73cgfrdw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# experiment tracking\n",
    "wandb.login()\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"biomed-bert-re\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": 1e-05,\n",
    "        'weight_decay': 0.0001,\n",
    "        'dropout_rate': 0.1,\n",
    "        \"architecture\": \"BRAN\",\n",
    "        \"dataset\": \"ChemDisGene\",\n",
    "        \"epochs\": 100,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc31d83c",
   "metadata": {},
   "source": [
    "## Batch processing of sequences and relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8817a22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1521/1521 [00:05<00:00, 298.27it/s]\n",
      "100%|██████████| 1939/1939 [00:07<00:00, 260.66it/s]\n",
      "100%|██████████| 523/523 [00:02<00:00, 255.22it/s]\n",
      "100%|██████████| 523/523 [00:02<00:00, 252.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from module.data_loader import Dataloader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract', use_fast=True)\n",
    "chemdisgene = Dataloader('./baseline/data', tokenizer, training=False, logger=logger, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "283c7d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input', 'pad', 'docid', 'input_length', 'label_vectors', 'label_names', 'e1_indicators', 'e2_indicators', 'e1s', 'e2s', 'e1_types', 'e2_types'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chemdisgene.val[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe738299",
   "metadata": {},
   "source": [
    "## Constructing a baseline BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e23a6a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthogonal pretrainer loss: 8.07e-10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "Model                                                   [2, 1, 512, 512, 15]      245,760\n",
       "├─BertModel: 1-1                                        [2, 768]                  --\n",
       "│    └─BertEmbeddings: 2-1                              [2, 512, 768]             --\n",
       "│    │    └─Embedding: 3-1                              [2, 512, 768]             23,440,896\n",
       "│    │    └─Embedding: 3-2                              [2, 512, 768]             1,536\n",
       "│    │    └─Embedding: 3-3                              [1, 512, 768]             393,216\n",
       "│    │    └─LayerNorm: 3-4                              [2, 512, 768]             1,536\n",
       "│    │    └─Dropout: 3-5                                [2, 512, 768]             --\n",
       "│    └─BertEncoder: 2-2                                 [2, 512, 768]             --\n",
       "│    │    └─ModuleList: 3-6                             --                        85,054,464\n",
       "│    └─BertPooler: 2-3                                  [2, 768]                  --\n",
       "│    │    └─Linear: 3-7                                 [2, 768]                  590,592\n",
       "│    │    └─Tanh: 3-8                                   [2, 768]                  --\n",
       "├─Linear: 1-2                                           [2, 512, 768]             590,592\n",
       "├─ReLU: 1-3                                             [2, 512, 768]             --\n",
       "├─Linear: 1-4                                           [2, 512, 128]             98,432\n",
       "├─Linear: 1-5                                           [2, 512, 768]             590,592\n",
       "├─ReLU: 1-6                                             [2, 512, 768]             --\n",
       "├─Linear: 1-7                                           [2, 512, 128]             98,432\n",
       "=========================================================================================================\n",
       "Total params: 111,106,048\n",
       "Trainable params: 111,106,048\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 221.33\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 867.18\n",
       "Params size (MB): 443.44\n",
       "Estimated Total Size (MB): 1310.63\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "from module.model import Model\n",
    "\n",
    "config = {'data_path': './baseline/data', 'learning_rate': 1e-05, 'mode': 'train', 'encoder_type': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
    "          'model': 'biaffine', 'output_path': '', 'load_path': '', 'multi_label': True, 'grad_accumulation_steps': 16, 'max_text_length': 512, \n",
    "          'dim': 128, 'weight_decay': 0.0001, 'dropout_rate': 0.1, 'max_grad_norm': 10.0, 'epochs': 10, 'patience': 5, 'log_interval': 0.25, \n",
    "          'warmup': -1.0, 'cuda': True}\n",
    "\n",
    "model = Model(config)\n",
    "\n",
    "summary(model, input_size=[(2, 512), (2, 512)], dtypes=['torch.IntTensor', 'torch.IntTensor'], device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66fdaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "pubmedbert = AutoModelForMaskedLM.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38347a7",
   "metadata": {},
   "source": [
    "## Training one epoch on biochemical relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4124c1c",
   "metadata": {},
   "source": [
    "Preload training data to send them to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e0ed94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam with integrated weight decay regularization\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-05,\n",
    "                  weight_decay=0.0001, eps=1e-8)\n",
    "\n",
    "# y is 1 or 0, x is 1-d logit\n",
    "criterion = torch.nn.BCEWithLogitsLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0615ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.to(device)\n",
    "\n",
    "model.head_layer0.to(device)\n",
    "model.head_layer1.to(device)\n",
    "model.tail_layer0.to(device)\n",
    "model.tail_layer1.to(device)\n",
    "\n",
    "model.biaffine_mat = torch.nn.Parameter(model.biaffine_mat.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "326a68f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "train_dataset = []\n",
    "for batch_num, return_data in enumerate(chemdisgene):\n",
    "\n",
    "    # Get the virtual memory status\n",
    "    memory_info = psutil.virtual_memory()\n",
    "\n",
    "    # Convert used memory from bytes to GB\n",
    "    used_memory_gb = memory_info.used / (1024 ** 3)\n",
    "\n",
    "    train_dataset.append(return_data[1])\n",
    "\n",
    "    if used_memory_gb>=24: # Break if more than 24 GB is collected\n",
    "        break\n",
    "\n",
    "    if batch_num>=10000: # Break if more than 1000 batches are collected\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "812acfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(input_ids, attention_mask, ep_masks):\n",
    "    pairwise_scores = model(input_ids, attention_mask)\n",
    "    ep_masks = ep_masks.unsqueeze(4)\n",
    "    pairwise_scores = pairwise_scores + ep_masks\n",
    "    pairwise_scores = torch.logsumexp(pairwise_scores, dim=[2,3])\n",
    "    outputs = pairwise_scores[:, :, :-1]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564e94aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "\n",
    "for epoch in tqdm(range(wandb.config.epochs), desc=\"Training\"):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # training the model\n",
    "    for batch_idx, batch in tqdm(enumerate(chemdisgene)):\n",
    "        (input_ids, attention_mask, ep_masks, e1_indicators, e2_indicators, label_arrays) = batch[0]\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        \n",
    "        ep_masks = ep_masks.to(device)\n",
    "        labels = label_arrays.to(device)\n",
    "\n",
    "        # reset gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # predict the relationship types between entity pairs\n",
    "        scores = model_forward(input_ids, attention_mask, ep_masks)\n",
    "\n",
    "        # binary cross entropy loss\n",
    "        loss = criterion(scores, labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward() \n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # track the loss of each training example for debugging\n",
    "        wandb.log({\"batch\": batch_idx, \"batch_loss\": loss.item()})\n",
    "    \n",
    "    train_loss /= len(chemdisgene.train)\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": train_loss})\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores, labels = [], []\n",
    "        # \n",
    "        for sample_idx, data in enumerate(chemdisgene.val):\n",
    "            # \n",
    "            input_ids = torch.tensor(data[\"input\"]).to(device)\n",
    "            attention_mask = torch.tensor(data[\"pad\"]).to(device)\n",
    "\n",
    "            e1_indicators_ = np.array(data[\"e1_indicators\"])\n",
    "            e2_indicators_ = np.array(data[\"e2_indicators\"])\n",
    "\n",
    "            ep_masks_ = []\n",
    "            for e1_indicator, e2_indicator in list(zip(list(e1_indicators_), list(e2_indicators_))):\n",
    "                    ep_mask_ = np.full(\n",
    "                            (512, 512), -1e20)\n",
    "                    ep_outer = 1 - np.outer(e1_indicator, e2_indicator)\n",
    "                    ep_mask_ = ep_mask_ * ep_outer\n",
    "                    ep_masks_.append(ep_mask_)\n",
    "            ep_masks_ = np.array(ep_masks_)\n",
    "\n",
    "            ep_masks = torch.tensor(\n",
    "                    np.array(ep_masks_), dtype=torch.float32).to(device)\n",
    "            label_array = torch.tensor(\n",
    "                    np.array(data[\"label_vectors\"]), dtype=torch.float32)\n",
    "            \n",
    "            # \n",
    "            pairwise = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
    "            pairwise = pairwise + ep_masks.unsqueeze(0).unsqueeze(4)\n",
    "            pairwise = torch.logsumexp(pairwise, dim=[2,3])\n",
    "            score = pairwise[:, :, :-1]\n",
    "\n",
    "            #\n",
    "            score = score.detach().cpu().numpy().squeeze(axis=0)\n",
    "            prediction = (score > np.zeros(14))\n",
    "\n",
    "            for j in range(len(prediction)):\n",
    "                predict_names = []\n",
    "                for k in list(np.where(prediction[j] == 1)[0]):\n",
    "                        predict_names.append(\n",
    "                        chemdisgene.relation_name[k])\n",
    "                label_names = []\n",
    "                for k in list(np.where(label_array[j] == 1)[0]):\n",
    "                        label_names.append(chemdisgene.relation_name[k])\n",
    "                score_dict = {}\n",
    "                for k, scr in enumerate(list(scores[j])):\n",
    "                        if k not in chemdisgene.relation_name:\n",
    "                                score_dict[\"NA\"] = float(scr)\n",
    "                        else:\n",
    "                                score_dict[chemdisgene.relation_name[k]] = float(\n",
    "                                    scr)\n",
    "            # \n",
    "            wandb.log({\"epoch\": epoch, \"sample_idx\": sample_idx, \"docid\": data['docid'],\n",
    "                       \"e1s\": data['e1s'], \"e2s\": data['e2s'], \"label_names\": label_names,\n",
    "                       \"predictions\": predict_names, \"scores\": score_dict})    \n",
    "        \n",
    "        scores = np.concatenate(scores, axis=0)\n",
    "        labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "        average_precision = metrics.average_precision_score(\n",
    "                labels.flatten(), scores.flatten())\n",
    "        \n",
    "        predictions = (scores > np.zeros(14))\n",
    "        predictions_categ = predictions\n",
    "\n",
    "        results = calculate_metrics(\n",
    "        predictions, predictions_categ, labels)\n",
    "        summary_metrics = {\n",
    "              \"average_precision\": average_precision,\n",
    "              \"micro_f1\":results['micro_f']\n",
    "        }\n",
    "        wandb.log({\"epoch\": 0} | summary_metrics | categ_metrics(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59cabc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'sample_idx': 0, 'docid': '26583456', 'e1s': ['MESH:D005283'], 'e2s': ['MESH:D012131'], 'label_name': [], 'prediction': [], 'probabilities': []}\n",
      "{'epoch': 0, 'sample_idx': 1, 'docid': '26766292', 'e1s': ['MESH:D000077185', 'MESH:D000077185', 'MESH:D008315', 'MESH:D008315', 'MESH:D012967', 'MESH:D012967', 'MESH:D006861', 'MESH:D006861', 'MESH:D011794', 'MESH:D011794', 'MESH:D059808', 'MESH:D059808'], 'e2s': ['MESH:D006984', 'MESH:D007249', 'MESH:D006984', 'MESH:D007249', 'MESH:D006984', 'MESH:D007249', 'MESH:D006984', 'MESH:D007249', 'MESH:D006984', 'MESH:D007249', 'MESH:D006984', 'MESH:D007249'], 'label_name': [], 'prediction': [], 'probabilities': []}\n",
      "{'epoch': 0, 'average_precision': 0.023809523809523808, 'micro_f1': 0.010928961748633882, 'precision_chem_disease:marker/mechanism': 0.07692307692307693, 'precision_chem_disease:therapeutic': 0.0, 'precision_chem_gene:increases^expression': 0.0, 'precision_chem_gene:decreases^expression': 0.0, 'precision_gene_disease:marker/mechanism': 0.0, 'precision_chem_gene:increases^activity': 0.0, 'precision_chem_gene:decreases^activity': 0.0, 'precision_chem_gene:increases^metabolic_processing': 0.0, 'precision_chem_gene:affects^binding': 0.0, 'precision_chem_gene:increases^transport': 0.0, 'precision_chem_gene:decreases^metabolic_processing': 0.0, 'precision_chem_gene:affects^localization': 0.0, 'precision_chem_gene:affects^expression': 0.0, 'precision_gene_disease:therapeutic': 0.0, 'recall_chem_disease:marker/mechanism': 1.0, 'recall_chem_disease:therapeutic': 0.0, 'recall_chem_gene:increases^expression': 0.0, 'recall_chem_gene:decreases^expression': 0.0, 'recall_gene_disease:marker/mechanism': 0.0, 'recall_chem_gene:increases^activity': 0.0, 'recall_chem_gene:decreases^activity': 0.0, 'recall_chem_gene:increases^metabolic_processing': 0.0, 'recall_chem_gene:affects^binding': 0.0, 'recall_chem_gene:increases^transport': 0.0, 'recall_chem_gene:decreases^metabolic_processing': 0.0, 'recall_chem_gene:affects^localization': 0.0, 'recall_chem_gene:affects^expression': 0.0, 'recall_gene_disease:therapeutic': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "scores = []\n",
    "labels = []\n",
    "\n",
    "for sample_idx, data in enumerate(chemdisgene.val[0:2]):\n",
    "    # Input data tensors\n",
    "    input_ids = torch.tensor(data[\"input\"]).to(device)\n",
    "    attention_mask = torch.tensor(data[\"pad\"]).to(device)\n",
    "\n",
    "    e1_indicators_ = np.array(data[\"e1_indicators\"])\n",
    "    e2_indicators_ = np.array(data[\"e2_indicators\"])\n",
    "\n",
    "    ep_masks_ = []\n",
    "    for e1_indicator, e2_indicator in list(zip(list(e1_indicators_), list(e2_indicators_))):\n",
    "        ep_mask_ = np.full((512, 512), -1e20)\n",
    "        ep_outer = 1 - np.outer(e1_indicator, e2_indicator)\n",
    "        ep_mask_ = ep_mask_ * ep_outer\n",
    "        ep_masks_.append(ep_mask_)\n",
    "    ep_masks_ = np.array(ep_masks_)\n",
    "\n",
    "    ep_masks = torch.tensor(np.array(ep_masks_), dtype=torch.float32).to(device)\n",
    "    label_array = torch.tensor(np.array(data[\"label_vectors\"]), dtype=torch.float32)\n",
    "\n",
    "    # Model prediction\n",
    "    pairwise = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
    "    pairwise = pairwise + ep_masks.unsqueeze(0).unsqueeze(4)\n",
    "    pairwise = torch.logsumexp(pairwise, dim=[2, 3])\n",
    "    score = pairwise[:, :, :-1]\n",
    "\n",
    "    # Process and extract predictions\n",
    "    score = score.detach().cpu().numpy().squeeze(axis=0)\n",
    "    label = label_array.cpu().numpy()\n",
    "    prediction = (score > np.zeros(14))\n",
    "\n",
    "    scores.append(score)\n",
    "    labels.append(label)\n",
    "\n",
    "    label_names = []\n",
    "    predict_names = []\n",
    "    for j in range(len(prediction)):\n",
    "        pass\n",
    "\n",
    "    label_names = []\n",
    "    predict_name = []\n",
    "    #for j in range(len(prediction)):\n",
    "    #    predict_names = []\n",
    "    #    for k in list(np.where(prediction[j] == 1)[0]):\n",
    "    #        predict_names.append(chemdisgene.relation_name[k])\n",
    "    #    label_names = []\n",
    "    #    for k in list(np.where(label_array[j] == 1)[0]):\n",
    "    #        label_names.append(chemdisgene.relation_name[k])\n",
    "    #    score_dict = {}\n",
    "    #    for k, scr in enumerate(list(score[j])):\n",
    "    #        if k not in chemdisgene.relation_name:\n",
    "    #            score_dict[\"NA\"] = float(scr)\n",
    "    #        else:\n",
    "    #            score_dict[chemdisgene.relation_name[k]] = float(scr)\n",
    "    \n",
    "    # get probabilities from sigmoid \n",
    "    probs = 1/(1 + np.exp(-score[0]))\n",
    "\n",
    "    print({\"epoch\": 0, \"sample_idx\": sample_idx, \"docid\": data['docid'], \"e1s\": data['e1s'],\n",
    "            \"e2s\": data['e2s'], \"label_name\": label_names, \"prediction\": predict_names,\n",
    "            \"probabilities\": []})\n",
    "    \n",
    "scores = np.concatenate(scores, axis=0)\n",
    "labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "average_precision = metrics.average_precision_score(\n",
    "    labels.flatten(), scores.flatten())\n",
    "\n",
    "predictions = (scores > np.zeros(14))\n",
    "predictions_categ = predictions\n",
    "\n",
    "results = calculate_metrics(\n",
    "    predictions, predictions_categ, labels)\n",
    "summary_metrics = {\n",
    "    \"average_precision\": average_precision,\n",
    "    \"micro_f1\":results['micro_f']\n",
    "}\n",
    "print({\"epoch\": 0} | summary_metrics | categ_metrics(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "642abb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'micro_p': 0.005494505494505495,\n",
       " 'micro_r': 1.0,\n",
       " 'micro_f': 0.010928961748633882,\n",
       " 'macro_p': 0.005494505494505495,\n",
       " 'macro_r': 0.07142857142857142,\n",
       " 'macro_f': 0.010204081632653062,\n",
       " 'categ_acc': 1.0,\n",
       " 'categ_macro_p': 0.07142857142857142,\n",
       " 'categ_macro_r': 0.07142857142857142,\n",
       " 'categ_macro_f': 0.07142857142857142,\n",
       " 'na_acc': 0.07692307692307693,\n",
       " 'not_na_p': 0.07692307692307693,\n",
       " 'not_na_r': 1.0,\n",
       " 'not_na_f': 0.14285714285714288,\n",
       " 'na_p': 0,\n",
       " 'na_r': 0.0,\n",
       " 'na_f': 0,\n",
       " 'per_rel_p': array([0.07692308, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ]),\n",
       " 'per_rel_r': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'per_rel_f': array([0.14285714, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ]),\n",
       " 'categ_per_rel_p': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'categ_per_rel_r': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'categ_per_rel_f': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53789652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█</td></tr><tr><td>batch_loss</td><td>▅▆▂▁▂▁▁▇▆▁█▄▂▆▃▁▇▂█▄▂▁▅▃▅▁▂▃▄▂▂▂▇▂▂▁▂▂▁▂</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>1637</td></tr><tr><td>batch_loss</td><td>0.00664</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>0.00124</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twilight-dawn-1</strong> at: <a href='https://wandb.ai/ramonreszat/biomed-bert-re/runs/73cgfrdw' target=\"_blank\">https://wandb.ai/ramonreszat/biomed-bert-re/runs/73cgfrdw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231019_002220-73cgfrdw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4471746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, predictions_categ, labels):\n",
    "    # Calculate metrics given prediction and labels\n",
    "    # predictions: (N, R), does not include NA in R\n",
    "    # labels: (N, R), one and zeros, does not include NA in R\n",
    "    # predictions_categ: (N, R), contains predictions for calculating performance of categorical classifier (exclude NA)\n",
    "    \n",
    "    TPs = predictions * labels  # (N, R)\n",
    "    TP = TPs.sum()\n",
    "    P = predictions.sum()\n",
    "    T = labels.sum()\n",
    "\n",
    "    micro_p = TP / P if P != 0 else 0\n",
    "    micro_r = TP / T if T != 0 else 0\n",
    "    micro_f = 2 * micro_p * micro_r / (micro_p + micro_r) if micro_p + micro_r > 0 else 0\n",
    "\n",
    "    categ_TPs = predictions_categ * labels\n",
    "    categ_TP = categ_TPs.sum()\n",
    "    # Excludes instances whose label is NA\n",
    "    categ_Ps = (predictions_categ * (labels.sum(1) > 0)[:, None])\n",
    "\n",
    "    categ_acc = categ_TP / T if T != 0 else 0\n",
    "\n",
    "    not_NA_Ps = (predictions.sum(1) > 0)\n",
    "    not_NA_Ts = (labels.sum(1) > 0)\n",
    "    not_NA_TPs = not_NA_Ps * not_NA_Ts\n",
    "    not_NA_P = not_NA_Ps.sum()\n",
    "    not_NA_T = not_NA_Ts.sum()\n",
    "    not_NA_TP = not_NA_TPs.sum()\n",
    "    not_NA_prec = not_NA_TP / not_NA_P if not_NA_P != 0 else 0\n",
    "    not_NA_recall = not_NA_TP / not_NA_T if not_NA_T != 0 else 0\n",
    "    not_NA_f = 2 * not_NA_prec * not_NA_recall / (not_NA_prec + not_NA_recall) if not_NA_prec + not_NA_recall > 0 else 0\n",
    "\n",
    "    not_NA_acc = (not_NA_Ps == not_NA_Ts).mean()\n",
    "\n",
    "    NA_Ps = (predictions.sum(1) == 0)\n",
    "    NA_Ts = (labels.sum(1) == 0)\n",
    "    NA_TPs = NA_Ps * NA_Ts\n",
    "    NA_P = NA_Ps.sum()\n",
    "    NA_T = NA_Ts.sum()\n",
    "    NA_TP = NA_TPs.sum()\n",
    "    NA_prec = NA_TP / NA_P if NA_P != 0 else 0\n",
    "    NA_recall = NA_TP / NA_T if NA_T != 0 else 0\n",
    "    NA_f = 2 * NA_prec * NA_recall / (NA_prec + NA_recall) if NA_prec + NA_recall > 0 else 0\n",
    "\n",
    "    per_rel_p = np.zeros(predictions.shape[1])\n",
    "    per_rel_r = np.zeros(predictions.shape[1])\n",
    "    per_rel_f = np.zeros(predictions.shape[1])\n",
    "    categ_per_rel_p = np.zeros(predictions.shape[1])\n",
    "    categ_per_rel_r = np.zeros(predictions.shape[1])\n",
    "    categ_per_rel_f = np.zeros(predictions.shape[1])\n",
    "    # Per-relation metrics:\n",
    "    for i in range(predictions.shape[1]):\n",
    "        TP_ = TPs[:, i].sum()\n",
    "        P_ = predictions[:, i].sum()\n",
    "        T_ = labels[:, i].sum()\n",
    "        categ_TP_ = categ_TPs[:, i].sum()\n",
    "        categ_P_ = categ_Ps[:, i].sum()\n",
    "\n",
    "        # If no such relation in the test data, recall = 0\n",
    "        per_rel_r[i] = TP_ / T_ if T_ != 0 else 0\n",
    "        categ_per_rel_r[i] = categ_TP_ / T_ if T_ != 0 else 0\n",
    "\n",
    "        # If no such relation in the prediction, precision = 0\n",
    "        per_rel_p[i] = TP_ / P_ if P_ != 0 else 0\n",
    "\n",
    "        # If no such relation in the prediction, precision = 0\n",
    "        categ_per_rel_p[i] = categ_TP_ / categ_P_ if categ_P_ != 0 else 0\n",
    "\n",
    "        per_rel_f[i] = 2 * per_rel_p[i] * per_rel_r[i] / (per_rel_p[i] + per_rel_r[i]) if per_rel_p[i] + per_rel_r[i] > 0 else 0\n",
    "\n",
    "        categ_per_rel_f[i] = 2 * categ_per_rel_p[i] * categ_per_rel_r[i] / (categ_per_rel_p[i] + categ_per_rel_r[i]) if categ_per_rel_p[i] + categ_per_rel_r[i] > 0 else 0\n",
    "\n",
    "    macro_p = per_rel_p.mean()\n",
    "    macro_r = per_rel_r.mean()\n",
    "    macro_f = per_rel_f.mean()\n",
    "\n",
    "    categ_macro_p = categ_per_rel_p.mean()\n",
    "    categ_macro_r = categ_per_rel_r.mean()\n",
    "    categ_macro_f = categ_per_rel_f.mean()\n",
    "\n",
    "    results = {\n",
    "        \"micro_p\": micro_p,\n",
    "        \"micro_r\": micro_r,\n",
    "        \"micro_f\": micro_f,\n",
    "        \"macro_p\": macro_p,\n",
    "        \"macro_r\": macro_r,\n",
    "        \"macro_f\": macro_f,\n",
    "        \"categ_acc\": categ_acc,\n",
    "        \"categ_macro_p\": categ_macro_p,\n",
    "        \"categ_macro_r\": categ_macro_r,\n",
    "        \"categ_macro_f\": categ_macro_f,\n",
    "        \"na_acc\": not_NA_acc,\n",
    "        \"not_na_p\": not_NA_prec,\n",
    "        \"not_na_r\": not_NA_recall,\n",
    "        \"not_na_f\": not_NA_f,\n",
    "        \"na_p\": NA_prec,\n",
    "        \"na_r\": NA_recall,\n",
    "        \"na_f\": NA_f,\n",
    "        \"per_rel_p\": per_rel_p,\n",
    "        \"per_rel_r\": per_rel_r,\n",
    "        \"per_rel_f\": per_rel_f,\n",
    "        \"categ_per_rel_p\": categ_per_rel_p,\n",
    "        \"categ_per_rel_r\": categ_per_rel_r,\n",
    "        \"categ_per_rel_f\": categ_per_rel_f,\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e52bf509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categ_metrics(results): \n",
    "    return {\n",
    "    \"precision_chem_disease:marker/mechanism\": results['per_rel_p'][0],\n",
    "    \"precision_chem_disease:therapeutic\": results['per_rel_p'][1],\n",
    "    \"precision_chem_gene:increases^expression\": results['per_rel_p'][2],\n",
    "    \"precision_chem_gene:decreases^expression\": results['per_rel_p'][3],\n",
    "    \"precision_gene_disease:marker/mechanism\": results['per_rel_p'][4],\n",
    "    \"precision_chem_gene:increases^activity\": results['per_rel_p'][5],\n",
    "    \"precision_chem_gene:decreases^activity\": results['per_rel_p'][6],\n",
    "    \"precision_chem_gene:increases^metabolic_processing\": results['per_rel_p'][7],\n",
    "    \"precision_chem_gene:affects^binding\": results['per_rel_p'][8],\n",
    "    \"precision_chem_gene:increases^transport\": results['per_rel_p'][9],\n",
    "    \"precision_chem_gene:decreases^metabolic_processing\": results['per_rel_p'][10],\n",
    "    \"precision_chem_gene:affects^localization\": results['per_rel_p'][11],\n",
    "    \"precision_chem_gene:affects^expression\": results['per_rel_p'][12],\n",
    "    \"precision_gene_disease:therapeutic\": results['per_rel_p'][13],\n",
    "    \"recall_chem_disease:marker/mechanism\": results['per_rel_r'][0],\n",
    "    \"recall_chem_disease:therapeutic\": results['per_rel_r'][1],\n",
    "    \"recall_chem_gene:increases^expression\": results['per_rel_r'][2],\n",
    "    \"recall_chem_gene:decreases^expression\": results['per_rel_r'][3],\n",
    "    \"recall_gene_disease:marker/mechanism\": results['per_rel_r'][4],\n",
    "    \"recall_chem_gene:increases^activity\": results['per_rel_r'][5],\n",
    "    \"recall_chem_gene:decreases^activity\": results['per_rel_r'][6],\n",
    "    \"recall_chem_gene:increases^metabolic_processing\": results['per_rel_r'][7],\n",
    "    \"recall_chem_gene:affects^binding\": results['per_rel_r'][8],\n",
    "    \"recall_chem_gene:increases^transport\": results['per_rel_r'][9],\n",
    "    \"recall_chem_gene:decreases^metabolic_processing\": results['per_rel_r'][10],\n",
    "    \"recall_chem_gene:affects^localization\": results['per_rel_r'][11],\n",
    "    \"recall_chem_gene:affects^expression\": results['per_rel_r'][12],\n",
    "    \"recall_gene_disease:therapeutic\": results['per_rel_r'][13]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3ef18f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
