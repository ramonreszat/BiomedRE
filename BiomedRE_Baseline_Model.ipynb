{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e45c6b2",
   "metadata": {},
   "source": [
    "# Biomedical Relation Extraction from Scientific Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068f741",
   "metadata": {},
   "source": [
    "Baseline BERT model to extract relationships from PubMed articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0824c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch, logging\n",
    "\n",
    "# fix random seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# CUDA device if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "# log level for experiment\n",
    "logger = logging.getLogger(\"BioRE\")\n",
    "\n",
    "# code for the baseline model\n",
    "sys.path.append(\"./baseline/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ba182f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mramonreszat\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ramon/Developer/BiomedRE/wandb/run-20231019_002220-73cgfrdw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ramonreszat/biomed-bert-re/runs/73cgfrdw' target=\"_blank\">twilight-dawn-1</a></strong> to <a href='https://wandb.ai/ramonreszat/biomed-bert-re' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ramonreszat/biomed-bert-re' target=\"_blank\">https://wandb.ai/ramonreszat/biomed-bert-re</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ramonreszat/biomed-bert-re/runs/73cgfrdw' target=\"_blank\">https://wandb.ai/ramonreszat/biomed-bert-re/runs/73cgfrdw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# experiment tracking\n",
    "wandb.login()\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"biomed-bert-re\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": 1e-05,\n",
    "        'weight_decay': 0.0001,\n",
    "        'dropout_rate': 0.1,\n",
    "        \"architecture\": \"BRAN\",\n",
    "        \"dataset\": \"ChemDisGene\",\n",
    "        \"epochs\": 100,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc31d83c",
   "metadata": {},
   "source": [
    "## Batch processing of sequences and relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8817a22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1521/1521 [00:04<00:00, 305.96it/s]\n",
      "100%|██████████| 1939/1939 [00:07<00:00, 265.62it/s]\n",
      "100%|██████████| 523/523 [00:02<00:00, 257.89it/s]\n",
      "100%|██████████| 523/523 [00:01<00:00, 264.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from module.data_loader import Dataloader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract', use_fast=True)\n",
    "chemdisgene = Dataloader('./baseline/data', tokenizer, training=False, logger=logger, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "283c7d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input', 'pad', 'docid', 'input_length', 'label_vectors', 'label_names', 'e1_indicators', 'e2_indicators', 'e1s', 'e2s', 'e1_types', 'e2_types'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chemdisgene.val[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe738299",
   "metadata": {},
   "source": [
    "## Constructing a baseline BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e23a6a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthogonal pretrainer loss: 4.67e-10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "Model                                                   [2, 1, 512, 512, 15]      245,760\n",
       "├─BertModel: 1-1                                        [2, 768]                  --\n",
       "│    └─BertEmbeddings: 2-1                              [2, 512, 768]             --\n",
       "│    │    └─Embedding: 3-1                              [2, 512, 768]             23,440,896\n",
       "│    │    └─Embedding: 3-2                              [2, 512, 768]             1,536\n",
       "│    │    └─Embedding: 3-3                              [1, 512, 768]             393,216\n",
       "│    │    └─LayerNorm: 3-4                              [2, 512, 768]             1,536\n",
       "│    │    └─Dropout: 3-5                                [2, 512, 768]             --\n",
       "│    └─BertEncoder: 2-2                                 [2, 512, 768]             --\n",
       "│    │    └─ModuleList: 3-6                             --                        85,054,464\n",
       "│    └─BertPooler: 2-3                                  [2, 768]                  --\n",
       "│    │    └─Linear: 3-7                                 [2, 768]                  590,592\n",
       "│    │    └─Tanh: 3-8                                   [2, 768]                  --\n",
       "├─Linear: 1-2                                           [2, 512, 768]             590,592\n",
       "├─ReLU: 1-3                                             [2, 512, 768]             --\n",
       "├─Linear: 1-4                                           [2, 512, 128]             98,432\n",
       "├─Linear: 1-5                                           [2, 512, 768]             590,592\n",
       "├─ReLU: 1-6                                             [2, 512, 768]             --\n",
       "├─Linear: 1-7                                           [2, 512, 128]             98,432\n",
       "=========================================================================================================\n",
       "Total params: 111,106,048\n",
       "Trainable params: 111,106,048\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 221.33\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 867.18\n",
       "Params size (MB): 443.44\n",
       "Estimated Total Size (MB): 1310.63\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "from module.model import Model\n",
    "\n",
    "config = {'data_path': './baseline/data', 'learning_rate': 1e-05, 'mode': 'train', 'encoder_type': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
    "          'model': 'biaffine', 'output_path': '', 'load_path': '', 'multi_label': True, 'grad_accumulation_steps': 16, 'max_text_length': 512, \n",
    "          'dim': 128, 'weight_decay': 0.0001, 'dropout_rate': 0.1, 'max_grad_norm': 10.0, 'epochs': 10, 'patience': 5, 'log_interval': 0.25, \n",
    "          'warmup': -1.0, 'cuda': True}\n",
    "\n",
    "model = Model(config)\n",
    "\n",
    "summary(model, input_size=[(2, 512), (2, 512)], dtypes=['torch.IntTensor', 'torch.IntTensor'], device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66fdaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "pubmedbert = AutoModelForMaskedLM.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38347a7",
   "metadata": {},
   "source": [
    "## Training one epoch on biochemical relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4124c1c",
   "metadata": {},
   "source": [
    "Preload training data to send them to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e0ed94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam with integrated weight decay regularization\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-05,\n",
    "                  weight_decay=0.0001, eps=1e-8)\n",
    "\n",
    "# y is 1 or 0, x is 1-d logit\n",
    "criterion = torch.nn.BCEWithLogitsLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0615ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.to(device)\n",
    "\n",
    "model.head_layer0.to(device)\n",
    "model.head_layer1.to(device)\n",
    "model.tail_layer0.to(device)\n",
    "model.tail_layer1.to(device)\n",
    "\n",
    "model.biaffine_mat = torch.nn.Parameter(model.biaffine_mat.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "326a68f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "train_dataset = []\n",
    "for batch_num, return_data in enumerate(chemdisgene):\n",
    "\n",
    "    # Get the virtual memory status\n",
    "    memory_info = psutil.virtual_memory()\n",
    "\n",
    "    # Convert used memory from bytes to GB\n",
    "    used_memory_gb = memory_info.used / (1024 ** 3)\n",
    "\n",
    "    train_dataset.append(return_data[1])\n",
    "\n",
    "    if used_memory_gb>=24: # Break if more than 24 GB is collected\n",
    "        break\n",
    "\n",
    "    if batch_num>=10000: # Break if more than 1000 batches are collected\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "812acfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(input_ids, attention_mask, ep_masks):\n",
    "    pairwise_scores = model(input_ids, attention_mask)\n",
    "    ep_masks = ep_masks.unsqueeze(4)\n",
    "    pairwise_scores = pairwise_scores + ep_masks\n",
    "    pairwise_scores = torch.logsumexp(pairwise_scores, dim=[2,3])\n",
    "    outputs = pairwise_scores[:, :, :-1]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564e94aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "\n",
    "for epoch in tqdm(range(wandb.config.epochs), desc=\"Training\"):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # training the model\n",
    "    for batch_idx, batch in tqdm(enumerate(chemdisgene)):\n",
    "        (input_ids, attention_mask, ep_masks, e1_indicators, e2_indicators, label_arrays) = batch[0]\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        \n",
    "        ep_masks = ep_masks.to(device)\n",
    "        labels = label_arrays.to(device)\n",
    "\n",
    "        # reset gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # predict the relationship types between entity pairs\n",
    "        scores = model_forward(input_ids, attention_mask, ep_masks)\n",
    "\n",
    "        # binary cross entropy loss\n",
    "        loss = criterion(scores, labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward() \n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # track the loss of each training example for debugging\n",
    "        wandb.log({\"batch\": batch_idx, \"batch_loss\": loss.item()})\n",
    "    \n",
    "    train_loss /= len(chemdisgene.train)\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": train_loss})\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores, labels = [], []\n",
    "        # \n",
    "        for sample_idx, data in enumerate(chemdisgene.val):\n",
    "            # \n",
    "            input_ids = torch.tensor(data[\"input\"]).to(device)\n",
    "            attention_mask = torch.tensor(data[\"pad\"]).to(device)\n",
    "\n",
    "            e1_indicators_ = np.array(data[\"e1_indicators\"])\n",
    "            e2_indicators_ = np.array(data[\"e2_indicators\"])\n",
    "\n",
    "            ep_masks_ = []\n",
    "            for e1_indicator, e2_indicator in list(zip(list(e1_indicators_), list(e2_indicators_))):\n",
    "                    ep_mask_ = np.full(\n",
    "                            (512, 512), -1e20)\n",
    "                    ep_outer = 1 - np.outer(e1_indicator, e2_indicator)\n",
    "                    ep_mask_ = ep_mask_ * ep_outer\n",
    "                    ep_masks_.append(ep_mask_)\n",
    "            ep_masks_ = np.array(ep_masks_)\n",
    "\n",
    "            ep_masks = torch.tensor(\n",
    "                    np.array(ep_masks_), dtype=torch.float32).to(device)\n",
    "            label_array = torch.tensor(\n",
    "                    np.array(data[\"label_vectors\"]), dtype=torch.float32)\n",
    "            \n",
    "            # \n",
    "            pairwise = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
    "            pairwise = pairwise + ep_masks.unsqueeze(0).unsqueeze(4)\n",
    "            pairwise = torch.logsumexp(pairwise, dim=[2,3])\n",
    "            score = pairwise[:, :, :-1]\n",
    "\n",
    "            #\n",
    "            score = score.detach().cpu().numpy().squeeze(axis=0)\n",
    "            prediction = (score > np.zeros(14))\n",
    "\n",
    "            for j in range(len(prediction)):\n",
    "                predict_names = []\n",
    "                for k in list(np.where(prediction[j] == 1)[0]):\n",
    "                        predict_names.append(\n",
    "                        chemdisgene.relation_name[k])\n",
    "                label_names = []\n",
    "                for k in list(np.where(label_array[j] == 1)[0]):\n",
    "                        label_names.append(chemdisgene.relation_name[k])\n",
    "                score_dict = {}\n",
    "                for k, scr in enumerate(list(scores[j])):\n",
    "                        if k not in chemdisgene.relation_name:\n",
    "                                score_dict[\"NA\"] = float(scr)\n",
    "                        else:\n",
    "                                score_dict[chemdisgene.relation_name[k]] = float(\n",
    "                                    scr)\n",
    "            # \n",
    "            wandb.log({\"sample_idx\": sample_idx, \"docid\": data['docid'], \"e1s\": data['e1s'], \n",
    "                       \"e2s\": data['e2s'], \"label_names\": label_names, \"predictions\": predict_names,\n",
    "                       \"scores\": score_dict})    \n",
    "        \n",
    "        scores = np.concatenate(scores, axis=0)\n",
    "        labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "        average_precision = metrics.average_precision_score(\n",
    "                labels.flatten(), scores.flatten())\n",
    "        wandb.log({\"epoch\": epoch, \"average_precision\": average_precision})\n",
    "        \n",
    "        \n",
    "\n",
    "        results = self.calculate_metrics(\n",
    "                predictions, predictions_categ, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59cabc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sample_idx': 0, 'docid': '26583456', 'e1s': ['MESH:D005283'], 'e2s': ['MESH:D012131'], 'label_names': [], 'predictions': ['chem_disease:marker/mechanism', 'chem_disease:therapeutic', 'chem_gene:increases^expression', 'chem_gene:decreases^expression', 'gene_disease:marker/mechanism', 'chem_gene:increases^activity', 'chem_gene:decreases^activity', 'chem_gene:increases^metabolic_processing', 'chem_gene:affects^binding', 'chem_gene:increases^transport', 'chem_gene:decreases^metabolic_processing', 'chem_gene:affects^localization', 'chem_gene:affects^expression', 'gene_disease:therapeutic'], 'scores': {'chem_disease:marker/mechanism': 0.6742985248565674, 'chem_disease:therapeutic': 0.6742985248565674, 'chem_gene:increases^expression': 0.6742985248565674, 'chem_gene:decreases^expression': 0.6742985248565674, 'gene_disease:marker/mechanism': 0.6742985248565674, 'chem_gene:increases^activity': 0.6742985248565674, 'chem_gene:decreases^activity': 0.6742985248565674, 'chem_gene:increases^metabolic_processing': 0.6742985248565674, 'chem_gene:affects^binding': 0.6742985248565674, 'chem_gene:increases^transport': 0.6742985248565674, 'chem_gene:decreases^metabolic_processing': 0.6742985248565674, 'chem_gene:affects^localization': 0.6742985248565674, 'chem_gene:affects^expression': 0.6742985248565674, 'gene_disease:therapeutic': 0.6742985248565674}}\n",
      "{'sample_idx': 1, 'docid': '26766292', 'e1s': ['MESH:D012967', 'MESH:D012967', 'MESH:D000077185', 'MESH:D000077185', 'MESH:D011794', 'MESH:D011794', 'MESH:D008315', 'MESH:D008315', 'MESH:D059808', 'MESH:D059808', 'MESH:D006861', 'MESH:D006861'], 'e2s': ['MESH:D006984', 'MESH:D007249', 'MESH:D006984', 'MESH:D007249', 'MESH:D006984', 'MESH:D007249', 'MESH:D006984', 'MESH:D007249', 'MESH:D006984', 'MESH:D007249', 'MESH:D006984', 'MESH:D007249'], 'label_names': [], 'predictions': ['chem_disease:marker/mechanism', 'chem_disease:therapeutic', 'chem_gene:increases^expression', 'chem_gene:decreases^expression', 'gene_disease:marker/mechanism', 'chem_gene:increases^activity', 'chem_gene:decreases^activity', 'chem_gene:increases^metabolic_processing', 'chem_gene:affects^binding', 'chem_gene:increases^transport', 'chem_gene:decreases^metabolic_processing', 'chem_gene:affects^localization', 'chem_gene:affects^expression', 'gene_disease:therapeutic'], 'scores': {'chem_disease:marker/mechanism': 3.1496496200561523, 'chem_disease:therapeutic': 3.1496496200561523, 'chem_gene:increases^expression': 3.1496496200561523, 'chem_gene:decreases^expression': 3.1496496200561523, 'gene_disease:marker/mechanism': 3.1496496200561523, 'chem_gene:increases^activity': 3.1496496200561523, 'chem_gene:decreases^activity': 3.1496496200561523, 'chem_gene:increases^metabolic_processing': 3.1496496200561523, 'chem_gene:affects^binding': 3.1496496200561523, 'chem_gene:increases^transport': 3.1496496200561523, 'chem_gene:decreases^metabolic_processing': 3.1496496200561523, 'chem_gene:affects^localization': 3.1496496200561523, 'chem_gene:affects^expression': 3.1496496200561523, 'gene_disease:therapeutic': 3.1496496200561523}}\n",
      "{'epoch': 0, 'average_precision': 0.03571428571428571}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "scores = []\n",
    "labels = []\n",
    "\n",
    "for sample_idx, data in enumerate(chemdisgene.val[0:2]):\n",
    "    # Input data tensors\n",
    "    input_ids = torch.tensor(data[\"input\"]).to(device)\n",
    "    attention_mask = torch.tensor(data[\"pad\"]).to(device)\n",
    "\n",
    "    e1_indicators_ = np.array(data[\"e1_indicators\"])\n",
    "    e2_indicators_ = np.array(data[\"e2_indicators\"])\n",
    "\n",
    "    ep_masks_ = []\n",
    "    for e1_indicator, e2_indicator in list(zip(list(e1_indicators_), list(e2_indicators_))):\n",
    "        ep_mask_ = np.full((512, 512), -1e20)\n",
    "        ep_outer = 1 - np.outer(e1_indicator, e2_indicator)\n",
    "        ep_mask_ = ep_mask_ * ep_outer\n",
    "        ep_masks_.append(ep_mask_)\n",
    "    ep_masks_ = np.array(ep_masks_)\n",
    "\n",
    "    ep_masks = torch.tensor(np.array(ep_masks_), dtype=torch.float32).to(device)\n",
    "    label_array = torch.tensor(np.array(data[\"label_vectors\"]), dtype=torch.float32)\n",
    "\n",
    "    # Model prediction\n",
    "    pairwise = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
    "    pairwise = pairwise + ep_masks.unsqueeze(0).unsqueeze(4)\n",
    "    pairwise = torch.logsumexp(pairwise, dim=[2, 3])\n",
    "    score = pairwise[:, :, :-1]\n",
    "\n",
    "    # Process and extract predictions\n",
    "    score = score.detach().cpu().numpy().squeeze(axis=0)\n",
    "    label = label_array.cpu().numpy()\n",
    "    prediction = (score > np.zeros(14))\n",
    "\n",
    "    scores.append(score)\n",
    "    labels.append(label)\n",
    "\n",
    "    for j in range(len(prediction)):\n",
    "        predict_names = []\n",
    "        for k in list(np.where(prediction[j] == 1)[0]):\n",
    "            predict_names.append(chemdisgene.relation_name[k])\n",
    "        label_names = []\n",
    "        for k in list(np.where(label_array[j] == 1)[0]):\n",
    "            label_names.append(chemdisgene.relation_name[k])\n",
    "        score_dict = {}\n",
    "        for k, scr in enumerate(list(score[j])):\n",
    "            if k not in chemdisgene.relation_name:\n",
    "                score_dict[\"NA\"] = float(scr)\n",
    "            else:\n",
    "                score_dict[chemdisgene.relation_name[k]] = float(scr)\n",
    "    \n",
    "    print({\"sample_idx\": sample_idx, \"docid\": data['docid'], \"e1s\": data['e1s'],\n",
    "               \"e2s\": data['e2s'], \"label_names\": label_names, \"predictions\": predict_names,\n",
    "               \"scores\": score_dict})\n",
    "    \n",
    "scores = np.concatenate(scores, axis=0)\n",
    "labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "average_precision = metrics.average_precision_score(\n",
    "    labels.flatten(), scores.flatten())\n",
    "print({\"epoch\": 0, \"average_precision\": average_precision})\n",
    "\n",
    "\n",
    "\n",
    "#results = self.calculate_metrics(\n",
    "#    predictions, predictions_categ, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53789652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█</td></tr><tr><td>batch_loss</td><td>▅▆▂▁▂▁▁▇▆▁█▄▂▆▃▁▇▂█▄▂▁▅▃▅▁▂▃▄▂▂▂▇▂▂▁▂▂▁▂</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>1637</td></tr><tr><td>batch_loss</td><td>0.00664</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>0.00124</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twilight-dawn-1</strong> at: <a href='https://wandb.ai/ramonreszat/biomed-bert-re/runs/73cgfrdw' target=\"_blank\">https://wandb.ai/ramonreszat/biomed-bert-re/runs/73cgfrdw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231019_002220-73cgfrdw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
