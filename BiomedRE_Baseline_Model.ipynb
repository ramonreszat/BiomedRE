{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e45c6b2",
   "metadata": {},
   "source": [
    "# Biomedical Relation Extraction from Scientific Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068f741",
   "metadata": {},
   "source": [
    "Baseline BERT model to extract relationships from PubMed articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0824c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch, logging\n",
    "\n",
    "# fix random seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# CUDA device if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "# log level for experiment\n",
    "logger = logging.getLogger(\"BioRE\")\n",
    "\n",
    "# code for the baseline model\n",
    "sys.path.append(\"./baseline/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ba182f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mramonreszat\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d85cb0b61e40b1999c10c551d04ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112231088918633, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ramon/Developer/BiomedRE/wandb/run-20231021_181341-fxmoljmd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ramonreszat/biomed-bert-re/runs/fxmoljmd' target=\"_blank\">lilac-cherry-11</a></strong> to <a href='https://wandb.ai/ramonreszat/biomed-bert-re' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ramonreszat/biomed-bert-re' target=\"_blank\">https://wandb.ai/ramonreszat/biomed-bert-re</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ramonreszat/biomed-bert-re/runs/fxmoljmd' target=\"_blank\">https://wandb.ai/ramonreszat/biomed-bert-re/runs/fxmoljmd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# experiment tracking\n",
    "wandb.login()\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"biomed-bert-re\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": 1e-05,\n",
    "        'weight_decay': 0.0001,\n",
    "        'dropout_rate': 0.1,\n",
    "        \"architecture\": \"BRAN\",\n",
    "        \"dataset\": \"ChemDisGene\",\n",
    "        \"epochs\": 10,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc31d83c",
   "metadata": {},
   "source": [
    "## Batch processing of sequences and relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8817a22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76942/76942 [05:08<00:00, 249.02it/s] \n",
      "100%|██████████| 1521/1521 [00:09<00:00, 159.17it/s]\n",
      "100%|██████████| 1939/1939 [00:11<00:00, 169.99it/s]\n",
      "100%|██████████| 523/523 [00:03<00:00, 144.52it/s]\n",
      "100%|██████████| 523/523 [00:03<00:00, 141.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from module.data_loader import Dataloader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract', use_fast=True)\n",
    "chemdisgene = Dataloader('./baseline/data', tokenizer, training=True, logger=logger, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "283c7d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input', 'pad', 'docid', 'input_length', 'label_vectors', 'label_names', 'e1_indicators', 'e2_indicators', 'e1s', 'e2s', 'e1_types', 'e2_types'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chemdisgene.val[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe738299",
   "metadata": {},
   "source": [
    "## Constructing a baseline BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e23a6a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthogonal pretrainer loss: 2.75e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "Model                                                   [2, 1, 512, 512, 15]      245,760\n",
       "├─BertModel: 1-1                                        [2, 768]                  --\n",
       "│    └─BertEmbeddings: 2-1                              [2, 512, 768]             --\n",
       "│    │    └─Embedding: 3-1                              [2, 512, 768]             23,440,896\n",
       "│    │    └─Embedding: 3-2                              [2, 512, 768]             1,536\n",
       "│    │    └─Embedding: 3-3                              [1, 512, 768]             393,216\n",
       "│    │    └─LayerNorm: 3-4                              [2, 512, 768]             1,536\n",
       "│    │    └─Dropout: 3-5                                [2, 512, 768]             --\n",
       "│    └─BertEncoder: 2-2                                 [2, 512, 768]             --\n",
       "│    │    └─ModuleList: 3-6                             --                        85,054,464\n",
       "│    └─BertPooler: 2-3                                  [2, 768]                  --\n",
       "│    │    └─Linear: 3-7                                 [2, 768]                  590,592\n",
       "│    │    └─Tanh: 3-8                                   [2, 768]                  --\n",
       "├─Linear: 1-2                                           [2, 512, 768]             590,592\n",
       "├─ReLU: 1-3                                             [2, 512, 768]             --\n",
       "├─Linear: 1-4                                           [2, 512, 128]             98,432\n",
       "├─Linear: 1-5                                           [2, 512, 768]             590,592\n",
       "├─ReLU: 1-6                                             [2, 512, 768]             --\n",
       "├─Linear: 1-7                                           [2, 512, 128]             98,432\n",
       "=========================================================================================================\n",
       "Total params: 111,106,048\n",
       "Trainable params: 111,106,048\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 221.33\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 867.18\n",
       "Params size (MB): 443.44\n",
       "Estimated Total Size (MB): 1310.63\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "from module.model import Model\n",
    "\n",
    "config = {'data_path': './baseline/data', 'learning_rate': 1e-05, 'mode': 'train', 'encoder_type': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
    "          'model': 'biaffine', 'output_path': '', 'load_path': '', 'multi_label': True, 'grad_accumulation_steps': 16, 'max_text_length': 512, \n",
    "          'dim': 128, 'weight_decay': 0.0001, 'dropout_rate': 0.1, 'max_grad_norm': 10.0, 'epochs': 10, 'patience': 5, 'log_interval': 0.25, \n",
    "          'warmup': -1.0, 'cuda': True}\n",
    "\n",
    "model = Model(config)\n",
    "\n",
    "summary(model, input_size=[(2, 512), (2, 512)], dtypes=['torch.IntTensor', 'torch.IntTensor'], device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38347a7",
   "metadata": {},
   "source": [
    "## Training one epoch on biochemical relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4124c1c",
   "metadata": {},
   "source": [
    "Preload training data to send them to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e0ed94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam with integrated weight decay regularization\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-05,\n",
    "                  weight_decay=0.0001, eps=1e-8)\n",
    "\n",
    "# y is 1 or 0, x is 1-d logit\n",
    "criterion = torch.nn.BCEWithLogitsLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0615ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.to(device)\n",
    "\n",
    "model.head_layer0.to(device)\n",
    "model.head_layer1.to(device)\n",
    "model.tail_layer0.to(device)\n",
    "model.tail_layer1.to(device)\n",
    "\n",
    "model.biaffine_mat = torch.nn.Parameter(model.biaffine_mat.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "326a68f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "train_dataset = []\n",
    "for batch_num, return_data in enumerate(chemdisgene):\n",
    "\n",
    "    # Get the virtual memory status\n",
    "    memory_info = psutil.virtual_memory()\n",
    "\n",
    "    # Convert used memory from bytes to GB\n",
    "    used_memory_gb = memory_info.used / (1024 ** 3)\n",
    "\n",
    "    train_dataset.append(return_data[1])\n",
    "\n",
    "    if used_memory_gb>=24: # Break if more than 24 GB is collected\n",
    "        break\n",
    "\n",
    "    if batch_num>=100: # Break if more than 1000 batches are collected\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f27c36e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [23:20:33<00:00, 8403.30s/it]  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "\n",
    "for epoch in tqdm(range(wandb.config.epochs), desc=\"Training\"):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_idx, batch in enumerate(chemdisgene):\n",
    "        (input_ids, attention_mask, ep_masks, e1_indicators, e2_indicators, label_arrays) = batch[1]\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        \n",
    "        ep_masks = ep_masks.to(device)\n",
    "        labels = label_arrays.to(device)\n",
    "\n",
    "        # reset gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # predict the relationship types between entity pairs\n",
    "        pairwise_scores = model(input_ids, attention_mask)\n",
    "        ep_masks = ep_masks.unsqueeze(4)\n",
    "        pairwise_scores = pairwise_scores + ep_masks\n",
    "        pairwise_scores = torch.logsumexp(pairwise_scores, dim=[2,3])\n",
    "        outputs = pairwise_scores[:, :, :-1]\n",
    "\n",
    "        # binary cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward() \n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # track the loss of each training example for debugging\n",
    "        wandb.log({\"batch\": batch_idx, \"batch_loss\": loss.item()})\n",
    "\n",
    "        if batch_idx % 10000 == 0:\n",
    "                with torch.no_grad():\n",
    "                        scores = []\n",
    "                        labels = []\n",
    "                        val_loss = 0.0\n",
    "                        # \n",
    "                        for sample_idx, data in enumerate(chemdisgene.val):\n",
    "                                # \n",
    "                                input_ids = torch.tensor(data[\"input\"]).to(device)\n",
    "                                attention_mask = torch.tensor(data[\"pad\"]).to(device)\n",
    "\n",
    "                                e1_indicators_ = np.array(data[\"e1_indicators\"])\n",
    "                                e2_indicators_ = np.array(data[\"e2_indicators\"])\n",
    "\n",
    "                                ep_masks_ = []\n",
    "                                for e1_indicator, e2_indicator in list(zip(list(e1_indicators_), list(e2_indicators_))):\n",
    "                                        ep_mask_ = np.full(\n",
    "                                                (512, 512), -1e20)\n",
    "                                        ep_outer = 1 - np.outer(e1_indicator, e2_indicator)\n",
    "                                        ep_mask_ = ep_mask_ * ep_outer\n",
    "                                        ep_masks_.append(ep_mask_)\n",
    "                                ep_masks_ = np.array(ep_masks_)\n",
    "\n",
    "                                ep_masks = torch.tensor(\n",
    "                                        np.array(ep_masks_), dtype=torch.float32).to(device)\n",
    "                                label_array = torch.tensor(\n",
    "                                        np.array(data[\"label_vectors\"]), dtype=torch.float32)\n",
    "                                \n",
    "                                # \n",
    "                                pairwise = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
    "                                pairwise = pairwise + ep_masks.unsqueeze(0).unsqueeze(4)\n",
    "                                pairwise = torch.logsumexp(pairwise, dim=[2,3])\n",
    "                                score = pairwise[:, :, :-1]\n",
    "\n",
    "                                loss = criterion(score.squeeze(axis=0), label_array.to(device))\n",
    "                                val_loss += loss.item()\n",
    "\n",
    "                                #\n",
    "                                score = score.detach().cpu().numpy().squeeze(axis=0)\n",
    "                                label = label_array.cpu().numpy()\n",
    "                                \n",
    "                                scores.append(score)\n",
    "                                labels.append(label)\n",
    "\n",
    "                                label_names = []\n",
    "                                predict_names = []\n",
    "                                for j in range(len(score)):\n",
    "                                        if np.all(label[j] == 0):\n",
    "                                                label_names.append('')\n",
    "                                        else:\n",
    "                                                label_names.append(\n",
    "                                                        chemdisgene.relation_name[\n",
    "                                                        np.where(label[j] == 1)[0][0]\n",
    "                                                        ])\n",
    "                                        predict_names.append(chemdisgene.relation_name[np.argmax(score[j])])\n",
    "\n",
    "                                wandb.log({\"epoch\": 0, \"sample_idx\": sample_idx, \"docid\": data['docid'], \"e1s\": data['e1s'],\n",
    "                                        \"e2s\": data['e2s'], \"prediction\": predict_names, \"labels\": label_names})\n",
    "                                \n",
    "                        scores = np.concatenate(scores, axis=0)\n",
    "                        labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "                        average_precision = metrics.average_precision_score(\n",
    "                        labels.flatten(), scores.flatten())\n",
    "\n",
    "                        predictions = (scores > np.zeros(14))\n",
    "                        predictions_categ = predictions\n",
    "\n",
    "                        results = calculate_metrics(\n",
    "                                predictions, predictions_categ, labels)\n",
    "                        summary_metrics = {\n",
    "                                \"average_precision\": average_precision,\n",
    "                                \"micro_f1\":results['micro_f']\n",
    "                                }\n",
    "                        val_loss /= len(chemdisgene.val)\n",
    "                        wandb.log({\"epoch\": 0, \"val_loss\": val_loss} | summary_metrics | categ_metrics(results))\n",
    "\n",
    "        if batch_idx>=len(chemdisgene.train)-1:\n",
    "                chemdisgene._idx=0\n",
    "                break\n",
    "    \n",
    "    train_loss /= len(chemdisgene.train)\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": train_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c4d5132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_precision</td><td>▁▆▇▇▇▇█████▇████▇████████▇███▇██████▇▇██</td></tr><tr><td>batch</td><td>▂▃▅▇▁▄▅▇▁▃▆▇▁▃▅█▂▃▅▇▂▄▅▇▁▄▆▇▁▃▆█▂▃▅█▂▄▅█</td></tr><tr><td>batch_loss</td><td>▃▁▂▂▁▆▆▁▄▂▅█▂▁▃▇▁▆▃▂▁▃▁▂▃▂▁▃▂▁▂▂▄▂▂▂█▃▁▂</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>micro_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision_chem_disease:marker/mechanism</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision_chem_disease:therapeutic</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision_chem_gene:affects^binding</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision_chem_gene:affects^expression</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision_chem_gene:affects^localization</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision_chem_gene:decreases^activity</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision_chem_gene:decreases^expression</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision_chem_gene:decreases^metabolic_processing</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision_chem_gene:increases^activity</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision_chem_gene:increases^expression</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision_chem_gene:increases^metabolic_processing</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision_chem_gene:increases^transport</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision_gene_disease:marker/mechanism</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision_gene_disease:therapeutic</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recall_chem_disease:marker/mechanism</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recall_chem_disease:therapeutic</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recall_chem_gene:affects^binding</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recall_chem_gene:affects^expression</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recall_chem_gene:affects^localization</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recall_chem_gene:decreases^activity</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recall_chem_gene:decreases^expression</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recall_chem_gene:decreases^metabolic_processing</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recall_chem_gene:increases^activity</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recall_chem_gene:increases^expression</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recall_chem_gene:increases^metabolic_processing</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recall_chem_gene:increases^transport</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recall_gene_disease:marker/mechanism</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recall_gene_disease:therapeutic</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sample_idx</td><td>▆▁▂▂▃▄▄▅▅▆▆▇▇█▃▄▅▅▆▆▇▇█▁▂▂▃▆▆▇▇█▁▂▂▃▃▄▄▅</td></tr><tr><td>val_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_precision</td><td>0.05044</td></tr><tr><td>batch</td><td>71858</td></tr><tr><td>batch_loss</td><td>0.02727</td></tr><tr><td>docid</td><td>33168510</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.05881</td></tr><tr><td>micro_f1</td><td>0</td></tr><tr><td>precision_chem_disease:marker/mechanism</td><td>0.0</td></tr><tr><td>precision_chem_disease:therapeutic</td><td>0.0</td></tr><tr><td>precision_chem_gene:affects^binding</td><td>0.0</td></tr><tr><td>precision_chem_gene:affects^expression</td><td>0.0</td></tr><tr><td>precision_chem_gene:affects^localization</td><td>0.0</td></tr><tr><td>precision_chem_gene:decreases^activity</td><td>0.0</td></tr><tr><td>precision_chem_gene:decreases^expression</td><td>0.0</td></tr><tr><td>precision_chem_gene:decreases^metabolic_processing</td><td>0.0</td></tr><tr><td>precision_chem_gene:increases^activity</td><td>0.0</td></tr><tr><td>precision_chem_gene:increases^expression</td><td>0.0</td></tr><tr><td>precision_chem_gene:increases^metabolic_processing</td><td>0.0</td></tr><tr><td>precision_chem_gene:increases^transport</td><td>0.0</td></tr><tr><td>precision_gene_disease:marker/mechanism</td><td>0.0</td></tr><tr><td>precision_gene_disease:therapeutic</td><td>0.0</td></tr><tr><td>recall_chem_disease:marker/mechanism</td><td>0.0</td></tr><tr><td>recall_chem_disease:therapeutic</td><td>0.0</td></tr><tr><td>recall_chem_gene:affects^binding</td><td>0.0</td></tr><tr><td>recall_chem_gene:affects^expression</td><td>0.0</td></tr><tr><td>recall_chem_gene:affects^localization</td><td>0.0</td></tr><tr><td>recall_chem_gene:decreases^activity</td><td>0.0</td></tr><tr><td>recall_chem_gene:decreases^expression</td><td>0.0</td></tr><tr><td>recall_chem_gene:decreases^metabolic_processing</td><td>0.0</td></tr><tr><td>recall_chem_gene:increases^activity</td><td>0.0</td></tr><tr><td>recall_chem_gene:increases^expression</td><td>0.0</td></tr><tr><td>recall_chem_gene:increases^metabolic_processing</td><td>0.0</td></tr><tr><td>recall_chem_gene:increases^transport</td><td>0.0</td></tr><tr><td>recall_gene_disease:marker/mechanism</td><td>0.0</td></tr><tr><td>recall_gene_disease:therapeutic</td><td>0.0</td></tr><tr><td>sample_idx</td><td>1479</td></tr><tr><td>val_loss</td><td>0.03928</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lilac-cherry-11</strong> at: <a href='https://wandb.ai/ramonreszat/biomed-bert-re/runs/fxmoljmd' target=\"_blank\">https://wandb.ai/ramonreszat/biomed-bert-re/runs/fxmoljmd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231021_181341-fxmoljmd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4471746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, predictions_categ, labels):\n",
    "    # Calculate metrics given prediction and labels\n",
    "    # predictions: (N, R), does not include NA in R\n",
    "    # labels: (N, R), one and zeros, does not include NA in R\n",
    "    # predictions_categ: (N, R), contains predictions for calculating performance of categorical classifier (exclude NA)\n",
    "    \n",
    "    TPs = predictions * labels  # (N, R)\n",
    "    TP = TPs.sum()\n",
    "    P = predictions.sum()\n",
    "    T = labels.sum()\n",
    "\n",
    "    micro_p = TP / P if P != 0 else 0\n",
    "    micro_r = TP / T if T != 0 else 0\n",
    "    micro_f = 2 * micro_p * micro_r / (micro_p + micro_r) if micro_p + micro_r > 0 else 0\n",
    "\n",
    "    categ_TPs = predictions_categ * labels\n",
    "    categ_TP = categ_TPs.sum()\n",
    "    # Excludes instances whose label is NA\n",
    "    categ_Ps = (predictions_categ * (labels.sum(1) > 0)[:, None])\n",
    "\n",
    "    categ_acc = categ_TP / T if T != 0 else 0\n",
    "\n",
    "    not_NA_Ps = (predictions.sum(1) > 0)\n",
    "    not_NA_Ts = (labels.sum(1) > 0)\n",
    "    not_NA_TPs = not_NA_Ps * not_NA_Ts\n",
    "    not_NA_P = not_NA_Ps.sum()\n",
    "    not_NA_T = not_NA_Ts.sum()\n",
    "    not_NA_TP = not_NA_TPs.sum()\n",
    "    not_NA_prec = not_NA_TP / not_NA_P if not_NA_P != 0 else 0\n",
    "    not_NA_recall = not_NA_TP / not_NA_T if not_NA_T != 0 else 0\n",
    "    not_NA_f = 2 * not_NA_prec * not_NA_recall / (not_NA_prec + not_NA_recall) if not_NA_prec + not_NA_recall > 0 else 0\n",
    "\n",
    "    not_NA_acc = (not_NA_Ps == not_NA_Ts).mean()\n",
    "\n",
    "    NA_Ps = (predictions.sum(1) == 0)\n",
    "    NA_Ts = (labels.sum(1) == 0)\n",
    "    NA_TPs = NA_Ps * NA_Ts\n",
    "    NA_P = NA_Ps.sum()\n",
    "    NA_T = NA_Ts.sum()\n",
    "    NA_TP = NA_TPs.sum()\n",
    "    NA_prec = NA_TP / NA_P if NA_P != 0 else 0\n",
    "    NA_recall = NA_TP / NA_T if NA_T != 0 else 0\n",
    "    NA_f = 2 * NA_prec * NA_recall / (NA_prec + NA_recall) if NA_prec + NA_recall > 0 else 0\n",
    "\n",
    "    per_rel_p = np.zeros(predictions.shape[1])\n",
    "    per_rel_r = np.zeros(predictions.shape[1])\n",
    "    per_rel_f = np.zeros(predictions.shape[1])\n",
    "    categ_per_rel_p = np.zeros(predictions.shape[1])\n",
    "    categ_per_rel_r = np.zeros(predictions.shape[1])\n",
    "    categ_per_rel_f = np.zeros(predictions.shape[1])\n",
    "    # Per-relation metrics:\n",
    "    for i in range(predictions.shape[1]):\n",
    "        TP_ = TPs[:, i].sum()\n",
    "        P_ = predictions[:, i].sum()\n",
    "        T_ = labels[:, i].sum()\n",
    "        categ_TP_ = categ_TPs[:, i].sum()\n",
    "        categ_P_ = categ_Ps[:, i].sum()\n",
    "\n",
    "        # If no such relation in the test data, recall = 0\n",
    "        per_rel_r[i] = TP_ / T_ if T_ != 0 else 0\n",
    "        categ_per_rel_r[i] = categ_TP_ / T_ if T_ != 0 else 0\n",
    "\n",
    "        # If no such relation in the prediction, precision = 0\n",
    "        per_rel_p[i] = TP_ / P_ if P_ != 0 else 0\n",
    "\n",
    "        # If no such relation in the prediction, precision = 0\n",
    "        categ_per_rel_p[i] = categ_TP_ / categ_P_ if categ_P_ != 0 else 0\n",
    "\n",
    "        per_rel_f[i] = 2 * per_rel_p[i] * per_rel_r[i] / (per_rel_p[i] + per_rel_r[i]) if per_rel_p[i] + per_rel_r[i] > 0 else 0\n",
    "\n",
    "        categ_per_rel_f[i] = 2 * categ_per_rel_p[i] * categ_per_rel_r[i] / (categ_per_rel_p[i] + categ_per_rel_r[i]) if categ_per_rel_p[i] + categ_per_rel_r[i] > 0 else 0\n",
    "\n",
    "    macro_p = per_rel_p.mean()\n",
    "    macro_r = per_rel_r.mean()\n",
    "    macro_f = per_rel_f.mean()\n",
    "\n",
    "    categ_macro_p = categ_per_rel_p.mean()\n",
    "    categ_macro_r = categ_per_rel_r.mean()\n",
    "    categ_macro_f = categ_per_rel_f.mean()\n",
    "\n",
    "    results = {\n",
    "        \"micro_p\": micro_p,\n",
    "        \"micro_r\": micro_r,\n",
    "        \"micro_f\": micro_f,\n",
    "        \"macro_p\": macro_p,\n",
    "        \"macro_r\": macro_r,\n",
    "        \"macro_f\": macro_f,\n",
    "        \"categ_acc\": categ_acc,\n",
    "        \"categ_macro_p\": categ_macro_p,\n",
    "        \"categ_macro_r\": categ_macro_r,\n",
    "        \"categ_macro_f\": categ_macro_f,\n",
    "        \"na_acc\": not_NA_acc,\n",
    "        \"not_na_p\": not_NA_prec,\n",
    "        \"not_na_r\": not_NA_recall,\n",
    "        \"not_na_f\": not_NA_f,\n",
    "        \"na_p\": NA_prec,\n",
    "        \"na_r\": NA_recall,\n",
    "        \"na_f\": NA_f,\n",
    "        \"per_rel_p\": per_rel_p,\n",
    "        \"per_rel_r\": per_rel_r,\n",
    "        \"per_rel_f\": per_rel_f,\n",
    "        \"categ_per_rel_p\": categ_per_rel_p,\n",
    "        \"categ_per_rel_r\": categ_per_rel_r,\n",
    "        \"categ_per_rel_f\": categ_per_rel_f,\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e52bf509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categ_metrics(results): \n",
    "    return {\n",
    "    \"precision_chem_disease:marker/mechanism\": results['per_rel_p'][0],\n",
    "    \"precision_chem_disease:therapeutic\": results['per_rel_p'][1],\n",
    "    \"precision_chem_gene:increases^expression\": results['per_rel_p'][2],\n",
    "    \"precision_chem_gene:decreases^expression\": results['per_rel_p'][3],\n",
    "    \"precision_gene_disease:marker/mechanism\": results['per_rel_p'][4],\n",
    "    \"precision_chem_gene:increases^activity\": results['per_rel_p'][5],\n",
    "    \"precision_chem_gene:decreases^activity\": results['per_rel_p'][6],\n",
    "    \"precision_chem_gene:increases^metabolic_processing\": results['per_rel_p'][7],\n",
    "    \"precision_chem_gene:affects^binding\": results['per_rel_p'][8],\n",
    "    \"precision_chem_gene:increases^transport\": results['per_rel_p'][9],\n",
    "    \"precision_chem_gene:decreases^metabolic_processing\": results['per_rel_p'][10],\n",
    "    \"precision_chem_gene:affects^localization\": results['per_rel_p'][11],\n",
    "    \"precision_chem_gene:affects^expression\": results['per_rel_p'][12],\n",
    "    \"precision_gene_disease:therapeutic\": results['per_rel_p'][13],\n",
    "    \"recall_chem_disease:marker/mechanism\": results['per_rel_r'][0],\n",
    "    \"recall_chem_disease:therapeutic\": results['per_rel_r'][1],\n",
    "    \"recall_chem_gene:increases^expression\": results['per_rel_r'][2],\n",
    "    \"recall_chem_gene:decreases^expression\": results['per_rel_r'][3],\n",
    "    \"recall_gene_disease:marker/mechanism\": results['per_rel_r'][4],\n",
    "    \"recall_chem_gene:increases^activity\": results['per_rel_r'][5],\n",
    "    \"recall_chem_gene:decreases^activity\": results['per_rel_r'][6],\n",
    "    \"recall_chem_gene:increases^metabolic_processing\": results['per_rel_r'][7],\n",
    "    \"recall_chem_gene:affects^binding\": results['per_rel_r'][8],\n",
    "    \"recall_chem_gene:increases^transport\": results['per_rel_r'][9],\n",
    "    \"recall_chem_gene:decreases^metabolic_processing\": results['per_rel_r'][10],\n",
    "    \"recall_chem_gene:affects^localization\": results['per_rel_r'][11],\n",
    "    \"recall_chem_gene:affects^expression\": results['per_rel_r'][12],\n",
    "    \"recall_gene_disease:therapeutic\": results['per_rel_r'][13]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185e6f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
