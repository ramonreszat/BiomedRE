{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e45c6b2",
   "metadata": {},
   "source": [
    "# Biomedical Relation Extraction from Scientific Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068f741",
   "metadata": {},
   "source": [
    "Baseline BERT model to extract relationships from PubMed articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0824c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch, logging\n",
    "\n",
    "# fix random seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# CUDA device if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "# log level for experiment\n",
    "logger = logging.getLogger(\"BioRE\")\n",
    "\n",
    "# code for the baseline model\n",
    "sys.path.append(\"./baseline/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ba182f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mramonreszat\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ramon/Developer/BiomedRE/wandb/run-20231019_002220-73cgfrdw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ramonreszat/biomed-bert-re/runs/73cgfrdw' target=\"_blank\">twilight-dawn-1</a></strong> to <a href='https://wandb.ai/ramonreszat/biomed-bert-re' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ramonreszat/biomed-bert-re' target=\"_blank\">https://wandb.ai/ramonreszat/biomed-bert-re</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ramonreszat/biomed-bert-re/runs/73cgfrdw' target=\"_blank\">https://wandb.ai/ramonreszat/biomed-bert-re/runs/73cgfrdw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# experiment tracking\n",
    "wandb.login()\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"biomed-bert-re\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": 1e-05,\n",
    "        'weight_decay': 0.0001,\n",
    "        'dropout_rate': 0.1,\n",
    "        \"architecture\": \"BRAN\",\n",
    "        \"dataset\": \"ChemDisGene\",\n",
    "        \"epochs\": 100,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5def47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cosmic-wood-1</strong> at: <a href='https://wandb.ai/ramonreszat/biomed-bert-re/runs/mw8ip1ci' target=\"_blank\">https://wandb.ai/ramonreszat/biomed-bert-re/runs/mw8ip1ci</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231018_224149-mw8ip1ci/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc31d83c",
   "metadata": {},
   "source": [
    "## Batch processing of sequences and relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8817a22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76942/76942 [05:17<00:00, 242.69it/s] \n",
      "100%|██████████| 1521/1521 [00:06<00:00, 227.13it/s]\n",
      "100%|██████████| 1939/1939 [00:14<00:00, 138.32it/s]\n",
      "100%|██████████| 523/523 [00:03<00:00, 132.50it/s]\n",
      "100%|██████████| 523/523 [00:03<00:00, 155.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from module.data_loader import Dataloader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract', use_fast=True)\n",
    "chemdisgene = Dataloader('./baseline/data', tokenizer, training=True, logger=logger, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c86e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "chemdisgene.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e87c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input', 'pad', 'docid', 'input_length', 'label_vectors', 'label_names', 'e1_indicators', 'e2_indicators', 'e1s', 'e2s', 'e1_types', 'e2_types'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chemdisgene.val[5].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d7b9025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_loader.val[5]['label_vectors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2177c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.val[5]['label_vectors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d58d8bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1480"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_loader.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe738299",
   "metadata": {},
   "source": [
    "## Constructing a baseline BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e23a6a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthogonal pretrainer loss: 1.68e-10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "Model                                                   [2, 1, 512, 512, 15]      245,760\n",
       "├─BertModel: 1-1                                        [2, 768]                  --\n",
       "│    └─BertEmbeddings: 2-1                              [2, 512, 768]             --\n",
       "│    │    └─Embedding: 3-1                              [2, 512, 768]             23,440,896\n",
       "│    │    └─Embedding: 3-2                              [2, 512, 768]             1,536\n",
       "│    │    └─Embedding: 3-3                              [1, 512, 768]             393,216\n",
       "│    │    └─LayerNorm: 3-4                              [2, 512, 768]             1,536\n",
       "│    │    └─Dropout: 3-5                                [2, 512, 768]             --\n",
       "│    └─BertEncoder: 2-2                                 [2, 512, 768]             --\n",
       "│    │    └─ModuleList: 3-6                             --                        85,054,464\n",
       "│    └─BertPooler: 2-3                                  [2, 768]                  --\n",
       "│    │    └─Linear: 3-7                                 [2, 768]                  590,592\n",
       "│    │    └─Tanh: 3-8                                   [2, 768]                  --\n",
       "├─Linear: 1-2                                           [2, 512, 768]             590,592\n",
       "├─ReLU: 1-3                                             [2, 512, 768]             --\n",
       "├─Linear: 1-4                                           [2, 512, 128]             98,432\n",
       "├─Linear: 1-5                                           [2, 512, 768]             590,592\n",
       "├─ReLU: 1-6                                             [2, 512, 768]             --\n",
       "├─Linear: 1-7                                           [2, 512, 128]             98,432\n",
       "=========================================================================================================\n",
       "Total params: 111,106,048\n",
       "Trainable params: 111,106,048\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 221.33\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 867.18\n",
       "Params size (MB): 443.44\n",
       "Estimated Total Size (MB): 1310.63\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "from module.model import Model\n",
    "\n",
    "config = {'data_path': './baseline/data', 'learning_rate': 1e-05, 'mode': 'train', 'encoder_type': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
    "          'model': 'biaffine', 'output_path': '', 'load_path': '', 'multi_label': True, 'grad_accumulation_steps': 16, 'max_text_length': 512, \n",
    "          'dim': 128, 'weight_decay': 0.0001, 'dropout_rate': 0.1, 'max_grad_norm': 10.0, 'epochs': 10, 'patience': 5, 'log_interval': 0.25, \n",
    "          'warmup': -1.0, 'cuda': True}\n",
    "\n",
    "model = Model(config)\n",
    "\n",
    "summary(model, input_size=[(2, 512), (2, 512)], dtypes=['torch.IntTensor', 'torch.IntTensor'], device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66fdaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "pubmedbert = AutoModelForMaskedLM.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38347a7",
   "metadata": {},
   "source": [
    "## Training one epoch on biochemical relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4124c1c",
   "metadata": {},
   "source": [
    "Preload training data to send them to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e0ed94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam with integrated weight decay regularization\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-05,\n",
    "                  weight_decay=0.0001, eps=1e-8)\n",
    "\n",
    "# y is 1 or 0, x is 1-d logit\n",
    "criterion = torch.nn.BCEWithLogitsLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0615ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.to(device)\n",
    "\n",
    "model.head_layer0.to(device)\n",
    "model.head_layer1.to(device)\n",
    "model.tail_layer0.to(device)\n",
    "model.tail_layer1.to(device)\n",
    "\n",
    "model.biaffine_mat = torch.nn.Parameter(model.biaffine_mat.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "326a68f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "train_dataset = []\n",
    "for batch_num, return_data in enumerate(chemdisgene):\n",
    "\n",
    "    # Get the virtual memory status\n",
    "    memory_info = psutil.virtual_memory()\n",
    "\n",
    "    # Convert used memory from bytes to GB\n",
    "    used_memory_gb = memory_info.used / (1024 ** 3)\n",
    "\n",
    "    train_dataset.append(return_data[1])\n",
    "\n",
    "    if used_memory_gb>=24: # Break if more than 24 GB is collected\n",
    "        break\n",
    "\n",
    "    if batch_num>=10000: # Break if more than 1000 batches are collected\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "812acfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(input_ids, attention_mask, ep_masks):\n",
    "    pairwise_scores = model(input_ids, attention_mask)\n",
    "    ep_masks = ep_masks.unsqueeze(4)\n",
    "    pairwise_scores = pairwise_scores + ep_masks\n",
    "    pairwise_scores = torch.logsumexp(pairwise_scores, dim=[2,3])\n",
    "    outputs = pairwise_scores[:, :, :-1]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "643dc576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24b95dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1638it [02:27, 11.07it/s]\n",
      "1638it [02:32, 10.74it/s]\n",
      "1638it [02:37, 10.43it/s]\n",
      "1638it [02:37, 10.43it/s]\n",
      "1638it [02:38, 10.35it/s]\n",
      "1638it [02:39, 10.25it/s]\n",
      "1638it [02:41, 10.17it/s]\n",
      "1638it [02:41, 10.17it/s]\n",
      "1638it [02:40, 10.20it/s]\n",
      "1638it [02:40, 10.21it/s]\n",
      "1638it [02:40, 10.20it/s]\n",
      "1638it [02:40, 10.22it/s]\n",
      "1638it [02:40, 10.20it/s]\n",
      "1638it [02:40, 10.19it/s]\n",
      "1638it [02:40, 10.20it/s]\n",
      "1638it [02:41, 10.14it/s]\n",
      "1638it [02:41, 10.17it/s]\n",
      "1638it [02:41, 10.14it/s]\n",
      "1638it [02:41, 10.14it/s]\n",
      "1638it [02:40, 10.22it/s]\n",
      "1638it [02:41, 10.16it/s]\n",
      "1638it [02:41, 10.14it/s]\n",
      "1638it [02:42, 10.09it/s]\n",
      "1638it [02:42, 10.10it/s]\n",
      "1638it [02:41, 10.14it/s]\n",
      "1638it [02:41, 10.12it/s]\n",
      "1638it [02:42, 10.09it/s]\n",
      "1638it [02:42, 10.08it/s]\n",
      "1638it [02:41, 10.14it/s]\n",
      "1638it [02:41, 10.14it/s]\n",
      "1638it [02:41, 10.14it/s]\n",
      "1638it [02:41, 10.15it/s]\n",
      "1638it [02:41, 10.13it/s]\n",
      "1638it [02:41, 10.14it/s]\n",
      "1638it [02:41, 10.12it/s]\n",
      "1638it [02:41, 10.11it/s]\n",
      "1638it [02:40, 10.21it/s]\n",
      "1638it [02:40, 10.19it/s]\n",
      "1638it [02:41, 10.14it/s]\n",
      "1638it [02:41, 10.12it/s]\n",
      "1638it [02:41, 10.13it/s]\n",
      "1638it [02:41, 10.11it/s]\n",
      "1638it [02:42, 10.09it/s]\n",
      "1638it [02:41, 10.14it/s]\n",
      "1638it [02:41, 10.14it/s]\n",
      "1638it [02:42, 10.10it/s]\n",
      "1638it [02:42, 10.10it/s]\n",
      "1638it [02:42, 10.09it/s]\n",
      "1638it [02:41, 10.13it/s]\n",
      "1638it [02:41, 10.15it/s]\n",
      "1638it [02:41, 10.15it/s]\n",
      "1638it [02:41, 10.12it/s]\n",
      "1638it [02:41, 10.12it/s]\n",
      "1638it [02:41, 10.15it/s]\n",
      "1638it [02:40, 10.18it/s]\n",
      "1638it [02:41, 10.15it/s]\n",
      "1638it [02:42, 10.11it/s]\n",
      "1638it [02:41, 10.16it/s]\n",
      "1638it [02:41, 10.16it/s]\n",
      "1638it [02:42, 10.10it/s]\n",
      "1638it [02:41, 10.15it/s]\n",
      "1638it [02:41, 10.15it/s]\n",
      "1638it [02:41, 10.16it/s]\n",
      "1638it [02:41, 10.17it/s]\n",
      "1638it [02:41, 10.12it/s]\n",
      "169it [00:16, 10.55it/s]wandb: Network error (ReadTimeout), entering retry loop.\n",
      "1638it [02:41, 10.16it/s]\n",
      "1638it [02:40, 10.21it/s]\n",
      "1638it [02:40, 10.23it/s]\n",
      "1638it [02:40, 10.23it/s]\n",
      "1638it [02:40, 10.23it/s]\n",
      "1638it [02:40, 10.17it/s]\n",
      "1638it [02:41, 10.15it/s]\n",
      "1638it [02:40, 10.18it/s]\n",
      "1638it [02:41, 10.15it/s]\n",
      "1638it [02:41, 10.14it/s]\n",
      "1638it [02:40, 10.19it/s]\n",
      "1638it [02:41, 10.14it/s]\n",
      "1638it [02:41, 10.17it/s]\n",
      "1638it [02:40, 10.18it/s]\n",
      "1638it [02:40, 10.20it/s]\n",
      "1638it [02:41, 10.17it/s]\n",
      "1638it [02:41, 10.15it/s]\n",
      "1638it [02:41, 10.12it/s]\n",
      "1638it [02:42, 10.07it/s]\n",
      "1638it [02:42, 10.08it/s]\n",
      "1638it [02:42, 10.08it/s]\n",
      "1638it [02:42, 10.07it/s]\n",
      "1638it [02:42, 10.07it/s]\n",
      "1638it [02:42, 10.09it/s]\n",
      "1638it [02:42, 10.06it/s]\n",
      "1638it [02:41, 10.13it/s]\n",
      "1638it [02:42, 10.11it/s]\n",
      "1638it [02:42, 10.09it/s]\n",
      "1638it [02:41, 10.11it/s]\n",
      "1638it [02:41, 10.13it/s]\n",
      "1638it [02:41, 10.12it/s]\n",
      "1638it [02:41, 10.15it/s]\n",
      "1638it [02:40, 10.21it/s]\n",
      "1638it [02:40, 10.20it/s]\n",
      "1638it [02:41, 10.16it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(wandb.config.epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    sample_loss = {}\n",
    "\n",
    "    for batch, return_data in tqdm(enumerate(train_dataset)):\n",
    "        (input_ids, attention_mask, ep_masks, e1_indicators, e2_indicators, label_arrays) = return_data\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        ep_masks = ep_masks.to(device)\n",
    "        labels = label_arrays.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model_forward(input_ids, attention_mask, ep_masks)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward() \n",
    "\n",
    "        optimizer.step()\n",
    "        wandb.log({\"batch\": batch, \"batch_loss\": loss.item()})\n",
    "    \n",
    "    train_loss /= len(chemdisgene.train)\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": train_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53789652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█▁█</td></tr><tr><td>batch_loss</td><td>▅▆▂▁▂▁▁▇▆▁█▄▂▆▃▁▇▂█▄▂▁▅▃▅▁▂▃▄▂▂▂▇▂▂▁▂▂▁▂</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>1637</td></tr><tr><td>batch_loss</td><td>0.00664</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>0.00124</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twilight-dawn-1</strong> at: <a href='https://wandb.ai/ramonreszat/biomed-bert-re/runs/73cgfrdw' target=\"_blank\">https://wandb.ai/ramonreszat/biomed-bert-re/runs/73cgfrdw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231019_002220-73cgfrdw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c69ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8289f368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ramon/Developer/BiomedRE/BiomedRE_Baseline_Model.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ramon/Developer/BiomedRE/BiomedRE_Baseline_Model.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_num, return_data \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(valid_loader)):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ramon/Developer/BiomedRE/BiomedRE_Baseline_Model.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m         (input_ids, attention_mask, ep_masks, e1_indicators, e2_indicators, label_arrays) \u001b[39m=\u001b[39m return_data[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/BiomedRE/baseline/src/module/data_loader.py:412\u001b[0m, in \u001b[0;36mDataloader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    410\u001b[0m     random\u001b[39m.\u001b[39mshuffle(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain)\n\u001b[1;32m    411\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 412\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_idx:(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_idx\u001b[39m+\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bz)][\u001b[39m0\u001b[39;49m]\n\u001b[1;32m    413\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bz\n\u001b[1;32m    414\u001b[0m input_array, pad_array, label_array, ep_masks, e1_indicators, e2_indicators \u001b[39m=\u001b[39m [\n\u001b[1;32m    415\u001b[0m ], [], [], [], [], []\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for batch_num, return_data in tqdm(enumerate(valid_loader)):\n",
    "        (input_array, attention_mask, ep_masks, e1_indicators, e2_indicators, label_arrays) = return_data[1]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d4eb623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.550358533859253"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loss[1560*N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c6a9bf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1039\n",
    "sample = [training_loss[1560*epoch+N] for epoch in range(epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2510ffc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07935695350170135,\n",
       " 0.07440420985221863,\n",
       " 0.07168351113796234,\n",
       " 0.0771072506904602,\n",
       " 0.07789056748151779,\n",
       " 0.06667759269475937,\n",
       " 0.06892801821231842,\n",
       " 0.06737707555294037,\n",
       " 0.06749895960092545,\n",
       " 0.06451743841171265]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a3a595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
