{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e45c6b2",
   "metadata": {},
   "source": [
    "# Biomedical Relation Extraction from Scientific Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068f741",
   "metadata": {},
   "source": [
    "This notebook describes a baseline BERT model that can be trained to extract relationships from PubMed articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0824c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch, logging\n",
    "\n",
    "# fix random seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# CUDA device if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "# log level for experiment\n",
    "logger = logging.getLogger(\"BioRE\")\n",
    "\n",
    "# code for the baseline model\n",
    "sys.path.append(\"./baseline/src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc31d83c",
   "metadata": {},
   "source": [
    "## Batch processing of sequences and relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8817a22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76942/76942 [05:35<00:00, 229.06it/s]\n",
      "100%|██████████| 1521/1521 [00:08<00:00, 178.18it/s]\n",
      "100%|██████████| 1939/1939 [00:13<00:00, 142.98it/s]\n",
      "100%|██████████| 523/523 [00:03<00:00, 145.47it/s]\n",
      "100%|██████████| 523/523 [00:03<00:00, 153.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from module.data_loader import Dataloader\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract', use_fast=True)\n",
    "\n",
    "# \n",
    "train_loader = Dataloader('./baseline/data', tokenizer, batch_size=2,\n",
    "        max_text_length=512, training=True, logger=logger, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d205eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "(num, return_data) = next(iter(train_loader))\n",
    "\n",
    "(input_ids, attention_mask, ep_masks, e1_indicators, e2_indicators, label_arrays) = return_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67cede7",
   "metadata": {},
   "source": [
    "One way to approach this might be to pad the number of entity pairs to *max_num_ep*. This would limit the number of relationships that can be retrieved per document, but would allow us to process multiple sequences per batch and *not waste valuable GPU cycles*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "788f0bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  3189,  5527,  ...,     0,     0,     3],\n",
       "        [    2, 21651,  4507,  ...,     0,     0,     3]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = input_ids.to(device)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee8edee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = attention_mask.to(device)\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35d241ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_masks = ep_masks.to(device)\n",
    "labels = label_arrays.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "224aada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put this in the data loader\n",
    "ep_masks = ep_masks.unsqueeze(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe738299",
   "metadata": {},
   "source": [
    "## Constructing a baseline BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e23a6a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthogonal pretrainer loss: 2.23e-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "Model                                                   [2, 1, 512, 512, 15]      245,760\n",
       "├─BertModel: 1-1                                        [2, 768]                  --\n",
       "│    └─BertEmbeddings: 2-1                              [2, 512, 768]             --\n",
       "│    │    └─Embedding: 3-1                              [2, 512, 768]             23,440,896\n",
       "│    │    └─Embedding: 3-2                              [2, 512, 768]             1,536\n",
       "│    │    └─Embedding: 3-3                              [1, 512, 768]             393,216\n",
       "│    │    └─LayerNorm: 3-4                              [2, 512, 768]             1,536\n",
       "│    │    └─Dropout: 3-5                                [2, 512, 768]             --\n",
       "│    └─BertEncoder: 2-2                                 [2, 512, 768]             --\n",
       "│    │    └─ModuleList: 3-6                             --                        85,054,464\n",
       "│    └─BertPooler: 2-3                                  [2, 768]                  --\n",
       "│    │    └─Linear: 3-7                                 [2, 768]                  590,592\n",
       "│    │    └─Tanh: 3-8                                   [2, 768]                  --\n",
       "├─Linear: 1-2                                           [2, 512, 768]             590,592\n",
       "├─ReLU: 1-3                                             [2, 512, 768]             --\n",
       "├─Linear: 1-4                                           [2, 512, 128]             98,432\n",
       "├─Linear: 1-5                                           [2, 512, 768]             590,592\n",
       "├─ReLU: 1-6                                             [2, 512, 768]             --\n",
       "├─Linear: 1-7                                           [2, 512, 128]             98,432\n",
       "=========================================================================================================\n",
       "Total params: 111,106,048\n",
       "Trainable params: 111,106,048\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 221.33\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 867.18\n",
       "Params size (MB): 443.44\n",
       "Estimated Total Size (MB): 1310.63\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "from module.model import Model\n",
    "\n",
    "# \n",
    "baseline = Model({'data_path': './baseline/data', 'learning_rate': 1e-05, 'mode': 'train', 'encoder_type': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
    "          'model': 'biaffine', 'output_path': '', 'load_path': '', 'multi_label': True, 'grad_accumulation_steps': 16, 'max_text_length': 512, \n",
    "          'dim': 128, 'weight_decay': 0.0001, 'dropout_rate': 0.1, 'max_grad_norm': 10.0, 'epochs': 10, 'patience': 5, 'log_interval': 0.25, \n",
    "          'warmup': -1.0, 'cuda': True})\n",
    "\n",
    "#\n",
    "pubmedbert = AutoModelForMaskedLM.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "\n",
    "\n",
    "summary(baseline, input_size=[(2, 512), (2, 512)], dtypes=['torch.IntTensor', 'torch.IntTensor'], device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2d9577",
   "metadata": {},
   "source": [
    "## Overfitting one batch of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fafeab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam with integrated weight decay regularization\n",
    "optimizer = torch.optim.AdamW(baseline.parameters(), lr=1e-05,\n",
    "                  weight_decay=0.0001, eps=1e-8)\n",
    "\n",
    "# y is 1 or 0, x is 1-d logit\n",
    "bcelogitloss = torch.nn.BCEWithLogitsLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7291ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "baseline.encoder.to(device)\n",
    "\n",
    "#\n",
    "baseline.head_layer0.to(device)\n",
    "baseline.head_layer1.to(device)\n",
    "baseline.tail_layer0.to(device)\n",
    "baseline.tail_layer1.to(device)\n",
    "\n",
    "#\n",
    "baseline.biaffine_mat = torch.nn.Parameter(baseline.biaffine_mat.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7a05808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:04<00:00,  2.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 10\n",
    "training_loss = []\n",
    "\n",
    "# training a single batch\n",
    "for i in tqdm(range(0, epochs), desc=\"Training\"):\n",
    "    \n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # pairwise relations (N, max_length, max_length, R+1)\n",
    "    pairwise_scores = baseline(input_ids, attention_mask)\n",
    "    \n",
    "    # broadcast predictions (N, 1, max_num_eps, R+1)\n",
    "    pairwise_scores = pairwise_scores + ep_masks\n",
    "    \n",
    "    # \n",
    "    pairwise_scores = torch.logsumexp(pairwise_scores, dim=[2,3])\n",
    "    \n",
    "    # multi-label logits (N, num_ep, R)\n",
    "    scores = pairwise_scores[:, :, :-1]\n",
    "    \n",
    "    # sigmoid and binary-cross entropy\n",
    "    loss = bcelogitloss(scores, labels)\n",
    "    loss.backward()\n",
    "    \n",
    "    # change weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # \n",
    "    training_loss.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95716059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0754338502883911,\n",
       " 0.8323897123336792,\n",
       " 0.6189757585525513,\n",
       " 0.43950217962265015,\n",
       " 0.3031347393989563,\n",
       " 0.2040204554796219,\n",
       " 0.12550131976604462,\n",
       " 0.07640735805034637,\n",
       " 0.04648725315928459,\n",
       " 0.031422968953847885]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5a2e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d22627be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: visualize prediction\n",
    "y_pred = torch.sigmoid(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38347a7",
   "metadata": {},
   "source": [
    "## Training one epoch on biochemical relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4124c1c",
   "metadata": {},
   "source": [
    "preprocess training data ready to be sent to the GPU in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326a68f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for batch_num, return_data in enumerate(train_loader):\n",
    "    (input_ids, attention_mask, ep_masks, e1_indicators, e2_indicators, label_arrays) = return_data[1]\n",
    "    train.append(input_ids)\n",
    "\n",
    "    # define training loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
